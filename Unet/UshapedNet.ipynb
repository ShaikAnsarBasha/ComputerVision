{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from zipfile import ZipFile\n\ntrain_path = \"/kaggle/input/carvana-image-masking-challenge/train.zip\"\ntrain_masks_path = \"/kaggle/input/carvana-image-masking-challenge/train_masks.zip\"\n\nwith ZipFile(train_path, 'r') as z:\n    z.extractall(path='./')\n    print(\"Done\")\n\nwith ZipFile(train_masks_path, 'r') as z:\n    z.extractall(path='./')\n    print(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2023-03-11T08:27:55.423829Z","iopub.execute_input":"2023-03-11T08:27:55.424506Z","iopub.status.idle":"2023-03-11T08:28:07.188871Z","shell.execute_reply.started":"2023-03-11T08:27:55.424474Z","shell.execute_reply":"2023-03-11T08:28:07.186850Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Done\nDone\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport os\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-03-11T08:49:34.258418Z","iopub.execute_input":"2023-03-11T08:49:34.258778Z","iopub.status.idle":"2023-03-11T08:49:34.267874Z","shell.execute_reply.started":"2023-03-11T08:49:34.258746Z","shell.execute_reply":"2023-03-11T08:49:34.266829Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class doubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(doubleConv, self).__init__()\n        self.same_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.same_conv(x)","metadata":{"execution":{"iopub.status.busy":"2023-03-11T08:49:38.413013Z","iopub.execute_input":"2023-03-11T08:49:38.413719Z","iopub.status.idle":"2023-03-11T08:49:38.420925Z","shell.execute_reply.started":"2023-03-11T08:49:38.413681Z","shell.execute_reply":"2023-03-11T08:49:38.419438Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class Unet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1, channels=[64, 128, 256, 512]):\n        super(Unet, self).__init__()\n        self.down_same_convs = nn.ModuleList()\n        self.up_same_convs = nn.ModuleList()\n\n        self.down_sample = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.up_samples = nn.ModuleList()\n        \n\n        for channel in channels:\n            down_same_conv = doubleConv(in_channels, channel)\n            self.down_same_convs.append(down_same_conv)\n            in_channels = channel\n\n        self.bottleNeck = doubleConv(channels[-1], channels[-1]*2)\n\n        channels = list(reversed(channels))\n        for channel in channels:\n            up_sample = nn.ConvTranspose2d(in_channels=channel*2, out_channels=channel, kernel_size=2, stride=2, padding=0)\n            up_same_conv = doubleConv(channel*2, channel)\n\n            self.up_samples.append(up_sample)\n            self.up_same_convs.append(up_same_conv)\n        \n        self.final_same_conv = nn.Conv2d(in_channels=channels[-1], out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n    \n    def forward(self, x):  \n        skip_connections = []\n        for down_same_conv in self.down_same_convs:\n            x = down_same_conv(x)\n            skip_connections.append(x)\n            x = self.down_sample(x)\n\n        x = self.bottleNeck(x)\n\n        skip_connections = list(reversed(skip_connections))\n        for i in range(4):\n            x = self.up_samples[i](x)\n            x = torch.cat((skip_connections[i], x), dim=1)\n            x = self.up_same_convs[i](x)\n\n        x = self.final_same_conv(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-03-11T08:49:38.925204Z","iopub.execute_input":"2023-03-11T08:49:38.926365Z","iopub.status.idle":"2023-03-11T08:49:39.077613Z","shell.execute_reply.started":"2023-03-11T08:49:38.926314Z","shell.execute_reply":"2023-03-11T08:49:39.076377Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"x = torch.randn((16, 3, 160, 240))\nunet = Unet(in_channels=3, out_channels=1)\nunet(x).shape","metadata":{"execution":{"iopub.status.busy":"2023-03-11T08:49:44.637883Z","iopub.execute_input":"2023-03-11T08:49:44.638584Z","iopub.status.idle":"2023-03-11T08:49:58.230427Z","shell.execute_reply.started":"2023-03-11T08:49:44.638538Z","shell.execute_reply":"2023-03-11T08:49:58.229263Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 1, 160, 240])"},"metadata":{}}]},{"cell_type":"code","source":"class CarvanaDataset:\n    def __init__(self, img_paths, transform=None):\n        self.img_paths = img_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, index):\n        img_path = self.img_paths[index]\n        mask_path = self.img_paths[index].replace(\"train\", \"train_masks\").replace(\".jpg\", \"_mask.gif\")\n        img = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n        mask[mask == 255.0] = 1.0\n\n        if self.transform is not None:\n            augmentations = self.transform(image=img, mask=mask)\n            img = augmentations[\"image\"]\n            mask = augmentations[\"mask\"]\n\n        return img, mask","metadata":{"execution":{"iopub.status.busy":"2023-03-11T08:50:02.436626Z","iopub.execute_input":"2023-03-11T08:50:02.437599Z","iopub.status.idle":"2023-03-11T08:50:02.445989Z","shell.execute_reply.started":"2023-03-11T08:50:02.437554Z","shell.execute_reply":"2023-03-11T08:50:02.444803Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"IMAGE_HEIGHT = 160\nIMAGE_WIDTH = 240\nBATCH_SIZE = 16\nNUM_WORKERS = 0\nPIN_MEMORY = False\n\ntrain_transform = A.Compose(\n    [\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Rotate(limit=35, p=1.0),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.1),\n        A.Normalize(\n            mean=[0.0, 0.0, 0.0],\n            std=[1.0, 1.0, 1.0],\n            max_pixel_value=255.0,\n        ),\n        ToTensorV2(),\n    ],\n)\n\nval_transform = A.Compose(\n    [\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Normalize(\n            mean=[0.0, 0.0, 0.0],\n            std=[1.0, 1.0, 1.0],\n            max_pixel_value=255.0,\n        ),\n        ToTensorV2(),\n    ],\n)\n\ntrain_img_paths = [os.path.join(\"./train\", img_name) for img_name in os.listdir(\"./train\")]\ntrain_dataset = CarvanaDataset(\n    train_img_paths,\n    transform=train_transform,\n)\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n    pin_memory=PIN_MEMORY,\n    shuffle=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-11T08:50:05.436642Z","iopub.execute_input":"2023-03-11T08:50:05.437777Z","iopub.status.idle":"2023-03-11T08:50:05.458557Z","shell.execute_reply.started":"2023-03-11T08:50:05.437728Z","shell.execute_reply":"2023-03-11T08:50:05.457620Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, filename=\"model.pth.tar\"):\n    print(\"=>Saving Checkpoint...\")\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    torch.save(checkpoint, filename)\n\ndef load_checkpoint(checkpoint_path, model, optimizer):\n    print(\"=>Loading Checkpoint...\")\n    checkpoint = torch.load(checkpoint_path, map_location=torch.device(DEVICE))\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-11T08:50:08.458534Z","iopub.execute_input":"2023-03-11T08:50:08.459248Z","iopub.status.idle":"2023-03-11T08:50:08.466450Z","shell.execute_reply.started":"2023-03-11T08:50:08.459212Z","shell.execute_reply":"2023-03-11T08:50:08.465352Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def evaluate(loader, model):\n    model.eval()\n    predictions = []\n    targets = []\n\n    for idx, (imgs, masks) in tqdm(enumerate(loader), total=len(loader), leave=True):\n        imgs= imgs.to(DEVICE)\n        masks = masks.to(DEVICE).unsqueeze(1)\n        \n        with torch.no_grad():\n            outputs = torch.sigmoid(model(imgs))\n            outputs = (outputs > 0.5)\n            \n        predictions.append(outputs)\n        targets.append(masks)\n    \n    predictions = torch.cat(predictions, dim=0).detach().cpu().numpy()\n    targets = torch.cat(targets, dim=0).detach().cpu().numpy()\n    \n    accuracy = metrics.accuracy_score(targets.reshape(-1), predictions.reshape(-1))\n    dice_score = (2 * (predictions * targets).sum()) / ((predictions + targets).sum() + 1e-8)\n    return accuracy, dice_score\n\ndef save_predictions_as_imgs(loader, model, epoch, folder=\"saved_imgs/\"):\n    model.eval()    \n    for idx, (imgs, masks) in enumerate(loader):\n        imgs = imgs.to(device=DEVICE)\n        masks = masks.to(device=DEVICE)\n        \n        with torch.no_grad():\n            outputs = torch.sigmoid(model(imgs))\n            outputs = (outputs > 0.5).type(torch.float)\n        torchvision.utils.save_image(masks.unsqueeze(1), f\"{folder}real_{epoch}.png\")\n        torchvision.utils.save_image(outputs, f\"{folder}/fake_{epoch}.png\")\n        break","metadata":{"execution":{"iopub.status.busy":"2023-03-11T08:50:26.199591Z","iopub.execute_input":"2023-03-11T08:50:26.200528Z","iopub.status.idle":"2023-03-11T08:50:26.212700Z","shell.execute_reply.started":"2023-03-11T08:50:26.200476Z","shell.execute_reply":"2023-03-11T08:50:26.211551Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def train(train_data_loader, model, criterion, optimizer, scaler, epoch):\n    model.train()\n    losses = []\n\n    train_progress_bar = tqdm(enumerate(train_data_loader), total=len(train_data_loader), leave=True)\n    for batch_idx, (imgs, masks) in train_progress_bar:\n        imgs = imgs.to(device=DEVICE)\n        masks = masks.unsqueeze(1).to(device=DEVICE)\n        \n        with torch.cuda.amp.autocast():\n            outputs = model(imgs)\n            loss = criterion(outputs, masks)\n        \n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_progress_bar.set_description(f\"Epoch [{epoch}/{epochs-1}]\")\n        train_progress_bar.set_postfix(train_loss=loss.item())\n        losses.append(loss.item())\n        \n    train_progress_bar.close()\n    return sum(losses) / len(losses)","metadata":{"execution":{"iopub.status.busy":"2023-03-11T08:50:29.831810Z","iopub.execute_input":"2023-03-11T08:50:29.832181Z","iopub.status.idle":"2023-03-11T08:50:29.841189Z","shell.execute_reply.started":"2023-03-11T08:50:29.832148Z","shell.execute_reply":"2023-03-11T08:50:29.840083Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nCHECKPOINT_FILE = \"unet.pth.tar\"\nLOAD_MODEL = False\nlearning_rate = 3e-4\nepochs = 3\n\nmodel = Unet(in_channels=3, out_channels=1).to(DEVICE)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nscaler = torch.cuda.amp.GradScaler()\n\nif LOAD_MODEL: \n    load_checkpoint(CHECKPOINT_FILE, model, optimizer)\n\nlosses = []\nfor epoch in range(epochs):\n    loss = train(train_data_loader, model, criterion, optimizer, scaler, epoch)\n    accuracy, dice_score = evaluate(train_data_loader, model)\n\n    save_checkpoint(model, optimizer, filename=CHECKPOINT_FILE)\n    save_predictions_as_imgs(train_data_loader, model, epoch)\n    print(f\"Epoch: {epoch}, loss: {loss}, accuracy: {accuracy}, dice_score: {dice_score}\")\n    losses.append(loss)","metadata":{"execution":{"iopub.status.busy":"2023-03-11T08:51:30.515241Z","iopub.execute_input":"2023-03-11T08:51:30.515595Z","iopub.status.idle":"2023-03-11T09:19:46.899256Z","shell.execute_reply.started":"2023-03-11T08:51:30.515563Z","shell.execute_reply":"2023-03-11T09:19:46.897253Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Epoch [0/2]: 100%|██████████| 318/318 [05:08<00:00,  1.03it/s, train_loss=0.0638]\n100%|██████████| 318/318 [03:48<00:00,  1.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"=>Saving Checkpoint...\nEpoch: 0, loss: 0.14234318689636463, accuracy: 0.9879480210790095, dice_score: 0.9716116068849536\n","output_type":"stream"},{"name":"stderr","text":"Epoch [1/2]: 100%|██████████| 318/318 [05:09<00:00,  1.03it/s, train_loss=0.0313]\n100%|██████████| 318/318 [03:49<00:00,  1.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"=>Saving Checkpoint...\nEpoch: 1, loss: 0.045289488175427016, accuracy: 0.9925582610636138, dice_score: 0.9823999350874817\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/2]: 100%|██████████| 318/318 [05:09<00:00,  1.03it/s, train_loss=0.0242]\n100%|██████████| 318/318 [03:49<00:00,  1.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"=>Saving Checkpoint...\nEpoch: 2, loss: 0.029736392299568502, accuracy: 0.9928924522159984, dice_score: 0.9832002340281316\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\nshutil.make_archive('./saved_imgs', 'zip', \"saved_imgs\")","metadata":{"execution":{"iopub.status.busy":"2023-03-11T09:26:25.917500Z","iopub.execute_input":"2023-03-11T09:26:25.917905Z","iopub.status.idle":"2023-03-11T09:26:25.930203Z","shell.execute_reply.started":"2023-03-11T09:26:25.917870Z","shell.execute_reply":"2023-03-11T09:26:25.929110Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/saved_imgs.zip'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}